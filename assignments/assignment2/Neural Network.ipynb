{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(os.path.join(\"..\", \"data\"), max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.474991, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302183, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302180, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302186, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302185, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302181, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302182, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302184, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302184, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302186, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302185, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302183, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302182, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302185, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302184, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302182, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302183, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302184, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302184, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302183, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD())\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down and train and val accuracy go up for every epoch\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17902298be0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def forward(self, x):\n",
    "        result = ... # промежуточные вычисления\n",
    "        self.x = x # сохраняем значения, которые нам\n",
    "                   # понадобятся при обратном проходе\n",
    "        return result\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        dx = ... # используем сохраненные значения, чтобы \n",
    "        dw = ... # вычислить градиент по x и по w\n",
    "        self.w.grad += dw # аккумулируем градиент dw\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.327447, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.317055, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.308875, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302413, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.297282, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.293190, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.289918, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.287288, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.285169, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.283454, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.282059, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.280917, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.279984, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.279217, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.278584, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.278058, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.277622, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.277257, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.276951, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.276693, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.327797, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.317322, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.309072, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302554, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.297383, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.293268, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.289971, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.287328, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.285198, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.283473, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.282073, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.280930, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.279995, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.279228, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.278595, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.278068, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.277631, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.277267, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.276960, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.276704, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.338513, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.320917, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.309354, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302162, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.291612, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.279376, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.272259, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.249074, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.224796, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.160366, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.077119, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.023314, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.990641, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.882378, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.865913, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.850085, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.830226, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.838123, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.812237, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.816095, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.761647, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.769044, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.771029, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 1.753963, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.715191, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.720617, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.700544, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.720224, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.739972, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.722064, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.677093, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.659074, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.643295, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.650655, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.648200, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.605776, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.613467, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.603818, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.607823, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.557173, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.571724, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.563613, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.535058, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.553322, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.521862, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.551279, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.533226, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.514212, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.489937, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.510153, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.522101, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.464724, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.432964, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.423723, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.432805, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.411636, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.431253, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.424014, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.431763, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.393281, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.386942, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.392408, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.389754, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.371687, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.383909, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.382041, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.392579, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.378158, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.377792, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.390032, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.372182, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.372677, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.375066, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.363849, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.382682, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.397308, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.371160, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.382351, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.391005, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.385355, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.347869, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.356205, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.400631, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.365190, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.349667, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.357898, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.365172, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.370781, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.346892, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.352720, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.341590, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.339883, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.344367, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.359988, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.343300, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.337527, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.345549, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.353123, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.332500, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.333938, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.330066, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.325719, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.328516, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.320576, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.324892, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.310105, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.320430, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.308837, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.296569, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.298800, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.305147, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.292964, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.294184, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.283020, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.286336, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.311874, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.307945, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.278573, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.274128, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.299602, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.308194, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.284072, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.268346, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.279644, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.261962, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.286428, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.256874, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.267382, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.266512, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.268048, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.267290, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.290552, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.297965, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.282717, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.277372, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.276435, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.265663, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.253758, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.259296, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.245549, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.263141, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.254392, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.265931, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.259925, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.262231, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.250383, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.259467, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.259928, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.245491, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.277881, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.334327, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.318011, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.307733, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.298739, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.292331, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.284000, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.263576, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.244180, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.216952, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.161041, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.071979, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.968128, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.895453, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.892852, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.902205, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.869949, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.840630, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.809441, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.812950, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.816037, Train accuracy: 0.400000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.318376, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302280, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.275931, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.244771, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.205394, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.121494, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.943060, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.803730, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 1.895753, Train accuracy: 0.533333, val accuracy: 0.133333\n",
      "Loss: 1.780033, Train accuracy: 0.466667, val accuracy: 0.133333\n",
      "Loss: 1.692183, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.633246, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.525975, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.377188, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.339075, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.268941, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.245105, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.221194, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.169602, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.096079, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 0.5*1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, MomentumSGD(momentum=0.3), learning_rate=4e-1, num_epochs=20, batch_size=100)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **40%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.333308, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333303, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333293, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333280, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333262, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333241, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333217, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333191, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333162, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333131, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333098, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333063, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333027, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.332989, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.332950, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.332910, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.332869, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.332828, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.332785, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.332741, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: 2.332697, Train accuracy: 0.200000, val accuracy: 0.200000\n",
      "Loss: 2.332653, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332608, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332562, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332517, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332470, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332424, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332377, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332330, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332283, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332236, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332188, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332141, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332093, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332045, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331997, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331949, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331901, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331853, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331805, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331757, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331709, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331661, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331613, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331565, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331517, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331470, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331422, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331374, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331326, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331278, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331231, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331183, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331135, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331088, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331040, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330993, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330945, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330898, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330851, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330804, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330757, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330709, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330662, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330616, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330569, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330522, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330475, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330428, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330382, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330335, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330289, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330242, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330196, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330150, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330104, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330058, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330011, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329965, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329920, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329874, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329828, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329782, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329737, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329691, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329645, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329600, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329555, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329509, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329464, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329419, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329374, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329329, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329284, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329239, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329194, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329149, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329105, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329060, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329015, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328971, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328926, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328882, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328838, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328794, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328749, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328705, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328661, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328617, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328573, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328530, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328486, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328442, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328399, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328355, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328311, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328268, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328225, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328181, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328138, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328095, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328052, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328009, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327966, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327923, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327880, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327837, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327795, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327752, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327709, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327667, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327624, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327582, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327539, Train accuracy: 0.200000, val accuracy: 0.133333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.327497, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327455, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327413, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327371, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327329, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327287, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327245, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327203, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327161, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327119, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327078, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327036, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326995, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326953, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326912, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326870, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326829, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326788, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326747, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326706, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326665, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326624, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326583, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326542, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326501, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326460, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326420, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326379, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326338, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326298, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326257, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326217, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326177, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326136, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326096, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326056, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326016, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325976, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325936, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325896, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325856, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325816, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325777, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325737, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325697, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325658, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325618, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325579, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325539, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325500, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325461, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325421, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325382, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325343, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325304, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325265, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325226, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325187, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325148, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325110, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325071, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325032, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.324994, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.324955, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.324917, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.324878, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "best validation accuracy achieved: 0.200000\n"
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rate = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, MomentumSGD(momentum=0.2), num_epochs, batch_size, learning_rate, learning_rate_decay)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "best_val_accuracy = np.max(val_history)  # FIXME\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x179020d3dd8>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'predict'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-83c278478367>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmulticlass_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Neural net test set accuracy: %f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'predict'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
