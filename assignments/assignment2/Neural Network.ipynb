{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(os.path.join(\"..\", \"data\"), max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.474403, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302185, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302186, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302183, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302185, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302182, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302184, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302186, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302186, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302182, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302184, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302182, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302183, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302180, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302183, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302181, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302185, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302185, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302185, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302183, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD())\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down and train and val accuracy go up for every epoch\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15a49076da0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def forward(self, x):\n",
    "        result = ... # промежуточные вычисления\n",
    "        self.x = x # сохраняем значения, которые нам\n",
    "                   # понадобятся при обратном проходе\n",
    "        return result\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        dx = ... # используем сохраненные значения, чтобы \n",
    "        dw = ... # вычислить градиент по x и по w\n",
    "        self.w.grad += dw # аккумулируем градиент dw\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.327408, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.317032, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.308858, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302395, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.297260, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.293176, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.289910, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.287280, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.285161, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.283444, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.282049, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.280911, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.279977, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.279211, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.278579, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.278054, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.277615, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.277253, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.276945, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.276688, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.327655, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.317196, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.308953, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302452, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.297297, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.293189, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.289910, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.287275, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.285152, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.283434, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.282036, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.280898, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.279963, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.279193, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.278562, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.278037, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.277599, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.277234, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.276930, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.276672, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.338326, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.320726, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.309230, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.300579, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.290241, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.279076, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.269464, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.250561, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.217019, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.156007, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.067278, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.002951, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.971993, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.911798, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.905534, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.845792, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.804663, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.785866, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.788834, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.790754, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.781048, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 1.754564, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.741077, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.744151, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.760742, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.756774, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.730167, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.726081, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.712432, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.684400, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.669895, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.664564, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.674410, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.671687, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.648932, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.609983, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.629422, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.593120, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.569683, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.591973, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.550994, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.526442, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.527991, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.515509, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.501427, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.479611, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.515307, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.466382, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.476952, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.447254, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.442250, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.437411, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.414870, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.417957, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.433834, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.391517, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.394108, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.397111, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.378153, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.380982, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.390437, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.387097, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.406708, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.387041, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.375803, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.378639, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.380097, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.395714, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.363203, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.372490, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.358886, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.382959, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.394723, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.367586, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.367077, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.368755, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.357915, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.371789, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.360214, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.373501, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.357797, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.377497, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.382670, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.372767, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.369174, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.351686, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.351114, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.366576, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.348050, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.362101, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.352334, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.342979, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.331005, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.337902, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.334659, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.361240, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.337281, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.321766, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.323676, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.323694, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.335401, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.317678, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.307315, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.309968, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.288400, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.299551, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.314715, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.305751, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.286192, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.286320, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.281689, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.287880, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.291048, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.281946, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.272138, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.274607, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.284217, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.277585, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.277135, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.267351, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.272641, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.270427, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.270550, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.271501, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.279608, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.268384, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.264770, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.269555, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.279670, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.273527, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.277544, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.278784, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.273244, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.249732, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.262129, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.281281, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.273871, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.283180, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.268177, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.261782, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.275212, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.264065, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.261238, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.262113, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.245412, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.270398, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.264456, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.262344, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.251151, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.254204, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.333018, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.323780, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.308140, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.300735, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.291549, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.283649, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.268543, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.245102, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.211148, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.159149, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.112967, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.032367, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.972028, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.942804, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.899489, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.809719, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.850539, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.902373, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.805924, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.780762, Train accuracy: 0.400000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.317983, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301965, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.275815, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.245223, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.207935, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.131988, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.970007, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.829584, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 1.880883, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.757226, Train accuracy: 0.466667, val accuracy: 0.133333\n",
      "Loss: 1.813920, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.627700, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.572237, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.528551, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.432315, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.285182, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.318993, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.312472, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.258885, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.140589, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 0.5*1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, MomentumSGD(momentum=0.3), learning_rate=4e-1, num_epochs=20, batch_size=100)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **40%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.333436, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333431, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333422, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333408, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333390, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333369, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333346, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333319, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333290, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333259, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333226, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333191, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333155, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333117, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333078, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.333038, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.332997, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.332955, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.332913, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.332869, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.332825, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332781, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332735, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332690, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332644, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332598, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332551, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332504, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332457, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332410, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332363, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332315, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332268, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332220, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332172, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332124, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332076, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.332028, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331980, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331932, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331884, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331836, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331788, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331740, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331692, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331644, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331596, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331548, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331500, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331452, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331404, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331356, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331309, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331261, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331213, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331166, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331071, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331024, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330976, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330929, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330882, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330835, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330788, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330741, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330694, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330647, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330600, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330553, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330506, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330460, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330413, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330367, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330320, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330274, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330228, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330182, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330135, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330089, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330043, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329997, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329952, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329906, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329860, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329814, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329769, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329723, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329678, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329632, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329587, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329542, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329497, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329451, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329406, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329361, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329317, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329272, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329227, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329182, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329138, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329093, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329048, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.329004, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328960, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328915, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328871, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328827, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328783, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328739, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328695, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328651, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328607, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328563, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328519, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328476, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328432, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328389, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328345, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328302, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328258, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328215, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328172, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328129, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328086, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328043, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328000, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327957, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327914, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327871, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327829, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327786, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327743, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327701, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327658, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327616, Train accuracy: 0.200000, val accuracy: 0.133333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.327574, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327531, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327489, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327447, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327405, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327363, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327321, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327279, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327237, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327195, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327154, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327112, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327070, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327029, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326987, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326946, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326905, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326863, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326822, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326781, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326740, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326699, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326658, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326617, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326576, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326535, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326494, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326454, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326413, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326373, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326332, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326292, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326251, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326211, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326171, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326130, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326090, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326050, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326010, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325970, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325930, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325890, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325850, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325811, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325771, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325731, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325692, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325652, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325613, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325573, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325534, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325495, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325455, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325416, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325377, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325338, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325299, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325260, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325221, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325182, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325143, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325105, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325066, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.325027, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.324989, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "best validation accuracy achieved: 0.133333\n"
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rate = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, MomentumSGD(momentum=0.2), num_epochs, batch_size, learning_rate, learning_rate_decay)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "best_val_accuracy = np.max(val_history)  # FIXME\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15a493b2400>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-83c278478367>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmulticlass_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Neural net test set accuracy: %f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
