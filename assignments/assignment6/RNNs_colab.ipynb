{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNNs_colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shereshevskiy/dlcourse_ai/blob/master/assignments/assignment6/RNNs_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDo27Tj0fjpB",
        "colab_type": "text"
      },
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0ptCPrSg80R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "7635b897-543f-4400-ae16-3a894d2ea276"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssSpxAexiINJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ffd675ec-673f-4d78-c82b-e841dc6a8d35"
      },
      "source": [
        "import os\n",
        "os.chdir(\"drive/My Drive/Colab Notebooks/dlcourse_ai/dlcourse_ai/assignments/assignment6\")\n",
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNNs_colab.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P59NYU98GCb9",
        "colab": {}
      },
      "source": [
        "# !pip3 -qq install torch==0.4.1\n",
        "# !pip3 -qq install bokeh==0.13.0\n",
        "# !pip3 -qq install gensim==3.6.0\n",
        "# !pip3 -qq install nltk\n",
        "# !pip3 -qq install scikit-learn==0.20.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8sVtGHmA9aBM",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-6CNKM3b4hT1"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O_XkoGNQUeGm"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QFEtWrS_4rUs"
      },
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EPIkKdFlHB-X"
      },
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TiA2dGmgF1rW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "2c7a1f80-a9aa-4036-e3ba-0d26316bb964"
      },
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d93g_swyJA_V"
      },
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QstS4NO0L97c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "7fd89d5d-c1cc-42f3-eeff-2c9c16088a19"
      },
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epdW8u_YXcAv"
      },
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xTai8Ta0lgwL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "568c93d8-a94a-4dd3-a408-c08971e5a1f8"
      },
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eChdLNGtXyP0"
      },
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pCjwwDs6Zq9x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ba146b6-2350-42ca-f9bf-e2da50205acf"
      },
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words in train = 45441. Tags = {'DET', 'ADP', 'PRON', 'NUM', 'PRT', 'X', 'ADJ', 'VERB', 'CONJ', 'ADV', 'NOUN', '.'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "URC1B2nvPGFt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "63ce86f3-860a-4131-d86b-4ff0193afebe"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmwAAAEyCAYAAABH+Yw/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkyRJ6pyBTZIkqXMGNkmSpM79f9IOfOQPZVj+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gArQwbzWWkgi"
      },
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5rWmSToIaeAo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c56c4775-5d83-4a5f-822b-538721bc2246"
      },
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "07Ymb_MkbWsF"
      },
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vjz_Rk0bbMyH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f01c3fcf-4f4b-4509-8a9e-c138a9032797"
      },
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uWMw6QHvbaDd"
      },
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8XCuxEBVbOY_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "448e6a74-43b8-4813-a405-32403fcbed6e"
      },
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4t3xyYd__8d-"
      },
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RtRbz1SwgEqc",
        "colab": {}
      },
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DhsTKZalfih6",
        "colab": {}
      },
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4XsRII5kW5x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e569e0ce-0083-4612-f173-925ae30232c5"
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 4), (32, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFnomDYNX_t6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "941e04a6-ab04-422a-eaf8-b71a1c0b1366"
      },
      "source": [
        "len(X_train)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36554"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07ZahpCSYOV0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "ab560ccf-f6df-4322-89e7-9f3fbfb2d525"
      },
      "source": [
        "y_batch"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[12,  9,  6, 10],\n",
              "        [ 9,  9, 11,  9],\n",
              "        [11,  8,  9,  6],\n",
              "        [10,  4,  1,  5],\n",
              "        [ 9,  6, 11,  8],\n",
              "        [ 8,  9,  9, 10],\n",
              "        [10, 12,  8,  9],\n",
              "        [ 9, 12,  9,  5],\n",
              "        [11, 11,  6,  7],\n",
              "        [ 9, 10, 10,  6],\n",
              "        [ 8,  6,  9,  8],\n",
              "        [ 4,  9, 11,  7],\n",
              "        [ 6,  8, 10,  3],\n",
              "        [ 8,  6,  5,  6],\n",
              "        [ 0,  2,  9,  1],\n",
              "        [ 0,  1,  8,  6],\n",
              "        [ 0,  5, 10,  3],\n",
              "        [ 0, 11,  9,  6],\n",
              "        [ 0, 10, 11,  5],\n",
              "        [ 0,  9, 10,  9],\n",
              "        [ 0, 11, 10, 11],\n",
              "        [ 0,  9,  9,  4],\n",
              "        [ 0,  8,  6,  8],\n",
              "        [ 0,  0,  6,  0],\n",
              "        [ 0,  0,  6,  0],\n",
              "        [ 0,  0,  6,  0],\n",
              "        [ 0,  0,  4,  0],\n",
              "        [ 0,  0,  1,  0],\n",
              "        [ 0,  0, 10,  0],\n",
              "        [ 0,  0,  5,  0],\n",
              "        [ 0,  0,  9,  0],\n",
              "        [ 0,  0,  8,  0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C5I9E9P6eFYv"
      },
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVEHju54d68T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # some example is here: https://pastebin.com/zXUiDjEE\n",
        "# # NOTE the variant with nonzero hidden\n",
        "\n",
        "# class LSTMTagger(nn.Module):\n",
        "#     def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "#         super().__init__()    \n",
        "#         self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        \n",
        "#         self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        \n",
        "#         self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim)\n",
        "        \n",
        "#         self.hidden2tag = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "#         self.hidden = self.init_hidden()\n",
        "        \n",
        "#     def init_hidden(self):\n",
        "#         # Before we've done anything, we dont have any hidden state.\n",
        "#         # Refer to the Pytorch documentation to see exactly\n",
        "#         # why they have this dimensionality.\n",
        "#         # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
        "#         return (torch.zeros(1, 1, self.lstm_hidden_dim),\n",
        "#                 torch.zeros(1, 1, self.lstm_hidden_dim))\n",
        "        \n",
        "\n",
        "#     def forward(self, inputs):\n",
        "#         embeds = self.word_embeddings(inputs)\n",
        "#         lstm_out, self.hidden = self.lstm(\n",
        "#             embeds.view(inputs.numel(), 1, -1), self.hidden)\n",
        "#         tag_space = self.hidden2tag(lstm_out.view(inputs.numel(), -1))\n",
        "#         tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "#         return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JMbQo_g4miH-",
        "colab": {}
      },
      "source": [
        "# see tutorial here: https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
        "\n",
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()    \n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        \n",
        "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        \n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim)\n",
        "        \n",
        "        self.hidden2tag = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "        \n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.word_embeddings(inputs)\n",
        "        lstm_out, _ = self.lstm(\n",
        "            embeds.view(inputs.numel(), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(inputs.numel(), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q_HA8zyheYGH"
      },
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbrxsZ2mehWB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a9f5c4c-c9f3-44ac-df6c-8a3ca079e2cd"
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "\n",
        "# TODO <calc accuracy>\n",
        "def get_accuracy(logits, y_batch):\n",
        "    labels = torch.argmax(logits.view(y_batch.size()[0], y_batch.size()[1], -1), dim=2)\n",
        "\n",
        "    accuracy = float(torch.sum(labels == y_batch)) / labels.numel()\n",
        "    return accuracy\n",
        "  \n",
        "accuracy = get_accuracy(logits, y_batch)\n",
        "accuracy"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0703125"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GMUyUm1hgpe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c7a04727-61fd-4827-836a-e412de85d154"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "loss = float(criterion(logits, y_batch.view(-1)))\n",
        "loss"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.531550407409668"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nSgV3NPUpcjH"
      },
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FprPQ0gllo7b",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "                loss = criterion(logits, y_batch.view(-1))  # my\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward(retain_graph=True)\n",
        "                    optimizer.step()\n",
        "\n",
        "                labels = torch.argmax(logits.view(y_batch.size()[0], y_batch.size()[1], -1), dim=2)  # my\n",
        "                cur_correct_count, cur_sum_count = float(torch.sum(labels == y_batch)), labels.numel()  # my\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6_ps7kWbz9w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1697
        },
        "outputId": "fdb2f6dd-64b1-40c8-df24-40ba8804be32"
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.32517, Accuracy = 90.41%: 100%|██████████| 572/572 [01:01<00:00, 11.28it/s]\n",
            "[1 / 50]   Val: Loss = 0.13879, Accuracy = 96.88%: 100%|██████████| 13/13 [00:07<00:00,  2.03it/s]\n",
            "[2 / 50] Train: Loss = 0.11149, Accuracy = 96.44%: 100%|██████████| 572/572 [01:00<00:00,  9.43it/s]\n",
            "[2 / 50]   Val: Loss = 0.11956, Accuracy = 97.71%: 100%|██████████| 13/13 [00:07<00:00,  1.96it/s]\n",
            "[3 / 50] Train: Loss = 0.08033, Accuracy = 97.38%: 100%|██████████| 572/572 [01:00<00:00,  9.50it/s]\n",
            "[3 / 50]   Val: Loss = 0.10987, Accuracy = 98.10%: 100%|██████████| 13/13 [00:08<00:00,  1.77it/s]\n",
            "[4 / 50] Train: Loss = 0.06511, Accuracy = 97.81%: 100%|██████████| 572/572 [00:59<00:00, 10.76it/s]\n",
            "[4 / 50]   Val: Loss = 0.10139, Accuracy = 98.31%: 100%|██████████| 13/13 [00:07<00:00,  1.86it/s]\n",
            "[5 / 50] Train: Loss = 0.05646, Accuracy = 98.04%: 100%|██████████| 572/572 [01:00<00:00, 11.16it/s]\n",
            "[5 / 50]   Val: Loss = 0.10841, Accuracy = 98.32%: 100%|██████████| 13/13 [00:07<00:00,  2.12it/s]\n",
            "[6 / 50] Train: Loss = 0.05074, Accuracy = 98.18%: 100%|██████████| 572/572 [00:59<00:00,  9.56it/s]\n",
            "[6 / 50]   Val: Loss = 0.11437, Accuracy = 98.32%: 100%|██████████| 13/13 [00:07<00:00,  2.33it/s]\n",
            "[7 / 50] Train: Loss = 0.04644, Accuracy = 98.30%: 100%|██████████| 572/572 [01:00<00:00,  9.45it/s]\n",
            "[7 / 50]   Val: Loss = 0.11551, Accuracy = 98.36%: 100%|██████████| 13/13 [00:07<00:00,  2.42it/s]\n",
            "[8 / 50] Train: Loss = 0.04324, Accuracy = 98.38%: 100%|██████████| 572/572 [01:01<00:00,  9.32it/s]\n",
            "[8 / 50]   Val: Loss = 0.10754, Accuracy = 98.45%: 100%|██████████| 13/13 [00:07<00:00,  2.15it/s]\n",
            "[9 / 50] Train: Loss = 0.04067, Accuracy = 98.45%: 100%|██████████| 572/572 [00:59<00:00,  9.62it/s]\n",
            "[9 / 50]   Val: Loss = 0.10609, Accuracy = 98.51%: 100%|██████████| 13/13 [00:08<00:00,  1.92it/s]\n",
            "[10 / 50] Train: Loss = 0.03856, Accuracy = 98.51%: 100%|██████████| 572/572 [00:59<00:00,  9.68it/s]\n",
            "[10 / 50]   Val: Loss = 0.11860, Accuracy = 98.38%: 100%|██████████| 13/13 [00:07<00:00,  1.91it/s]\n",
            "[11 / 50] Train: Loss = 0.03743, Accuracy = 98.53%: 100%|██████████| 572/572 [01:00<00:00,  9.53it/s]\n",
            "[11 / 50]   Val: Loss = 0.11212, Accuracy = 98.51%: 100%|██████████| 13/13 [00:07<00:00,  1.82it/s]\n",
            "[12 / 50] Train: Loss = 0.03660, Accuracy = 98.55%: 100%|██████████| 572/572 [01:00<00:00,  9.39it/s]\n",
            "[12 / 50]   Val: Loss = 0.10653, Accuracy = 98.54%: 100%|██████████| 13/13 [00:07<00:00,  1.87it/s]\n",
            "[13 / 50] Train: Loss = 0.03588, Accuracy = 98.57%: 100%|██████████| 572/572 [00:59<00:00, 11.55it/s]\n",
            "[13 / 50]   Val: Loss = 0.11023, Accuracy = 98.54%: 100%|██████████| 13/13 [00:07<00:00,  1.87it/s]\n",
            "[14 / 50] Train: Loss = 0.03478, Accuracy = 98.60%: 100%|██████████| 572/572 [01:00<00:00,  9.51it/s]\n",
            "[14 / 50]   Val: Loss = 0.10833, Accuracy = 98.55%: 100%|██████████| 13/13 [00:07<00:00,  1.85it/s]\n",
            "[15 / 50] Train: Loss = 0.03427, Accuracy = 98.61%: 100%|██████████| 572/572 [00:59<00:00, 12.15it/s]\n",
            "[15 / 50]   Val: Loss = 0.10911, Accuracy = 98.49%: 100%|██████████| 13/13 [00:07<00:00,  1.69it/s]\n",
            "[16 / 50] Train: Loss = 0.03374, Accuracy = 98.62%: 100%|██████████| 572/572 [00:59<00:00, 11.16it/s]\n",
            "[16 / 50]   Val: Loss = 0.11108, Accuracy = 98.51%: 100%|██████████| 13/13 [00:08<00:00,  1.59it/s]\n",
            "[17 / 50] Train: Loss = 0.03342, Accuracy = 98.62%: 100%|██████████| 572/572 [01:00<00:00,  9.49it/s]\n",
            "[17 / 50]   Val: Loss = 0.10980, Accuracy = 98.55%: 100%|██████████| 13/13 [00:07<00:00,  1.78it/s]\n",
            "[18 / 50] Train: Loss = 0.03299, Accuracy = 98.64%: 100%|██████████| 572/572 [00:59<00:00,  9.58it/s]\n",
            "[18 / 50]   Val: Loss = 0.11702, Accuracy = 98.53%: 100%|██████████| 13/13 [00:07<00:00,  2.26it/s]\n",
            "[19 / 50] Train: Loss = 0.03269, Accuracy = 98.64%: 100%|██████████| 572/572 [00:59<00:00, 11.02it/s]\n",
            "[19 / 50]   Val: Loss = 0.11853, Accuracy = 98.55%: 100%|██████████| 13/13 [00:07<00:00,  1.80it/s]\n",
            "[20 / 50] Train: Loss = 0.03237, Accuracy = 98.65%: 100%|██████████| 572/572 [01:00<00:00, 10.98it/s]\n",
            "[20 / 50]   Val: Loss = 0.11382, Accuracy = 98.58%: 100%|██████████| 13/13 [00:07<00:00,  1.96it/s]\n",
            "[21 / 50] Train: Loss = 0.03215, Accuracy = 98.66%: 100%|██████████| 572/572 [01:01<00:00,  9.13it/s]\n",
            "[21 / 50]   Val: Loss = 0.10763, Accuracy = 98.60%: 100%|██████████| 13/13 [00:08<00:00,  2.11it/s]\n",
            "[22 / 50] Train: Loss = 0.03203, Accuracy = 98.66%: 100%|██████████| 572/572 [00:59<00:00,  9.68it/s]\n",
            "[22 / 50]   Val: Loss = 0.11784, Accuracy = 98.56%: 100%|██████████| 13/13 [00:08<00:00,  1.90it/s]\n",
            "[23 / 50] Train: Loss = 0.03170, Accuracy = 98.67%: 100%|██████████| 572/572 [00:58<00:00, 11.60it/s]\n",
            "[23 / 50]   Val: Loss = 0.11144, Accuracy = 98.59%: 100%|██████████| 13/13 [00:07<00:00,  1.99it/s]\n",
            "[24 / 50] Train: Loss = 0.03149, Accuracy = 98.68%: 100%|██████████| 572/572 [01:00<00:00,  9.51it/s]\n",
            "[24 / 50]   Val: Loss = 0.11790, Accuracy = 98.53%: 100%|██████████| 13/13 [00:07<00:00,  2.04it/s]\n",
            "[25 / 50] Train: Loss = 0.03170, Accuracy = 98.67%: 100%|██████████| 572/572 [00:59<00:00, 10.53it/s]\n",
            "[25 / 50]   Val: Loss = 0.12296, Accuracy = 98.54%: 100%|██████████| 13/13 [00:07<00:00,  2.06it/s]\n",
            "[26 / 50] Train: Loss = 0.03129, Accuracy = 98.68%: 100%|██████████| 572/572 [01:00<00:00,  9.46it/s]\n",
            "[26 / 50]   Val: Loss = 0.13077, Accuracy = 98.47%: 100%|██████████| 13/13 [00:07<00:00,  2.04it/s]\n",
            "[27 / 50] Train: Loss = 0.03109, Accuracy = 98.69%: 100%|██████████| 572/572 [01:00<00:00,  9.52it/s]\n",
            "[27 / 50]   Val: Loss = 0.12592, Accuracy = 98.55%: 100%|██████████| 13/13 [00:07<00:00,  2.09it/s]\n",
            "[28 / 50] Train: Loss = 0.03151, Accuracy = 98.67%: 100%|██████████| 572/572 [00:59<00:00,  9.66it/s]\n",
            "[28 / 50]   Val: Loss = 0.12001, Accuracy = 98.47%: 100%|██████████| 13/13 [00:07<00:00,  1.71it/s]\n",
            "[29 / 50] Train: Loss = 0.03124, Accuracy = 98.68%: 100%|██████████| 572/572 [00:58<00:00,  9.71it/s]\n",
            "[29 / 50]   Val: Loss = 0.12019, Accuracy = 98.58%: 100%|██████████| 13/13 [00:08<00:00,  1.58it/s]\n",
            "[30 / 50] Train: Loss = 0.03122, Accuracy = 98.69%: 100%|██████████| 572/572 [00:59<00:00,  9.62it/s]\n",
            "[30 / 50]   Val: Loss = 0.12844, Accuracy = 98.55%: 100%|██████████| 13/13 [00:07<00:00,  1.70it/s]\n",
            "[31 / 50] Train: Loss = 0.03074, Accuracy = 98.70%: 100%|██████████| 572/572 [01:01<00:00,  9.49it/s]\n",
            "[31 / 50]   Val: Loss = 0.13643, Accuracy = 98.40%: 100%|██████████| 13/13 [00:07<00:00,  2.23it/s]\n",
            "[32 / 50] Train: Loss = 0.03120, Accuracy = 98.68%: 100%|██████████| 572/572 [00:59<00:00,  9.62it/s]\n",
            "[32 / 50]   Val: Loss = 0.12442, Accuracy = 98.51%: 100%|██████████| 13/13 [00:07<00:00,  1.85it/s]\n",
            "[33 / 50] Train: Loss = 0.03104, Accuracy = 98.68%: 100%|██████████| 572/572 [00:59<00:00,  9.66it/s]\n",
            "[33 / 50]   Val: Loss = 0.12999, Accuracy = 98.49%: 100%|██████████| 13/13 [00:07<00:00,  2.22it/s]\n",
            "[34 / 50] Train: Loss = 0.03078, Accuracy = 98.69%: 100%|██████████| 572/572 [01:00<00:00,  9.53it/s]\n",
            "[34 / 50]   Val: Loss = 0.13562, Accuracy = 98.47%: 100%|██████████| 13/13 [00:07<00:00,  1.89it/s]\n",
            "[35 / 50] Train: Loss = 0.03082, Accuracy = 98.70%: 100%|██████████| 572/572 [00:59<00:00,  9.55it/s]\n",
            "[35 / 50]   Val: Loss = 0.14206, Accuracy = 98.43%: 100%|██████████| 13/13 [00:08<00:00,  1.91it/s]\n",
            "[36 / 50] Train: Loss = 0.03067, Accuracy = 98.69%: 100%|██████████| 572/572 [00:59<00:00, 10.57it/s]\n",
            "[36 / 50]   Val: Loss = 0.13579, Accuracy = 98.54%: 100%|██████████| 13/13 [00:07<00:00,  2.28it/s]\n",
            "[37 / 50] Train: Loss = 0.03068, Accuracy = 98.70%: 100%|██████████| 572/572 [00:59<00:00,  9.58it/s]\n",
            "[37 / 50]   Val: Loss = 0.14198, Accuracy = 98.57%: 100%|██████████| 13/13 [00:07<00:00,  1.93it/s]\n",
            "[38 / 50] Train: Loss = 0.03088, Accuracy = 98.69%: 100%|██████████| 572/572 [00:59<00:00,  9.55it/s]\n",
            "[38 / 50]   Val: Loss = 0.13555, Accuracy = 98.55%: 100%|██████████| 13/13 [00:07<00:00,  1.75it/s]\n",
            "[39 / 50] Train: Loss = 0.03078, Accuracy = 98.69%: 100%|██████████| 572/572 [00:59<00:00,  9.61it/s]\n",
            "[39 / 50]   Val: Loss = 0.12979, Accuracy = 98.54%: 100%|██████████| 13/13 [00:07<00:00,  1.73it/s]\n",
            "[40 / 50] Train: Loss = 0.03034, Accuracy = 98.71%: 100%|██████████| 572/572 [01:01<00:00,  9.35it/s]\n",
            "[40 / 50]   Val: Loss = 0.14387, Accuracy = 98.50%: 100%|██████████| 13/13 [00:07<00:00,  1.82it/s]\n",
            "[41 / 50] Train: Loss = 0.03016, Accuracy = 98.71%: 100%|██████████| 572/572 [00:58<00:00,  9.70it/s]\n",
            "[41 / 50]   Val: Loss = 0.14864, Accuracy = 98.53%: 100%|██████████| 13/13 [00:08<00:00,  1.75it/s]\n",
            "[42 / 50] Train: Loss = 0.03030, Accuracy = 98.71%: 100%|██████████| 572/572 [00:58<00:00,  9.74it/s]\n",
            "[42 / 50]   Val: Loss = 0.14519, Accuracy = 98.54%: 100%|██████████| 13/13 [00:07<00:00,  1.98it/s]\n",
            "[43 / 50] Train: Loss = 0.03033, Accuracy = 98.71%: 100%|██████████| 572/572 [01:00<00:00,  9.46it/s]\n",
            "[43 / 50]   Val: Loss = 0.14924, Accuracy = 98.48%: 100%|██████████| 13/13 [00:07<00:00,  1.83it/s]\n",
            "[44 / 50] Train: Loss = 0.03021, Accuracy = 98.71%: 100%|██████████| 572/572 [01:00<00:00,  9.42it/s]\n",
            "[44 / 50]   Val: Loss = 0.15656, Accuracy = 98.43%: 100%|██████████| 13/13 [00:07<00:00,  2.10it/s]\n",
            "[45 / 50] Train: Loss = 0.03022, Accuracy = 98.71%: 100%|██████████| 572/572 [00:59<00:00, 10.61it/s]\n",
            "[45 / 50]   Val: Loss = 0.15980, Accuracy = 98.45%: 100%|██████████| 13/13 [00:07<00:00,  2.08it/s]\n",
            "[46 / 50] Train: Loss = 0.03017, Accuracy = 98.71%: 100%|██████████| 572/572 [00:59<00:00, 10.25it/s]\n",
            "[46 / 50]   Val: Loss = 0.15969, Accuracy = 98.45%: 100%|██████████| 13/13 [00:07<00:00,  2.15it/s]\n",
            "[47 / 50] Train: Loss = 0.03011, Accuracy = 98.72%: 100%|██████████| 572/572 [00:58<00:00,  9.70it/s]\n",
            "[47 / 50]   Val: Loss = 0.15029, Accuracy = 98.54%: 100%|██████████| 13/13 [00:08<00:00,  1.63it/s]\n",
            "[48 / 50] Train: Loss = 0.03017, Accuracy = 98.71%: 100%|██████████| 572/572 [00:58<00:00,  9.76it/s]\n",
            "[48 / 50]   Val: Loss = 0.14941, Accuracy = 98.57%: 100%|██████████| 13/13 [00:07<00:00,  2.29it/s]\n",
            "[49 / 50] Train: Loss = 0.02992, Accuracy = 98.72%: 100%|██████████| 572/572 [01:01<00:00,  9.35it/s]\n",
            "[49 / 50]   Val: Loss = 0.16301, Accuracy = 98.52%: 100%|██████████| 13/13 [00:07<00:00,  1.88it/s]\n",
            "[50 / 50] Train: Loss = 0.03000, Accuracy = 98.71%: 100%|██████████| 572/572 [00:59<00:00,  9.56it/s]\n",
            "[50 / 50]   Val: Loss = 0.15279, Accuracy = 98.52%: 100%|██████████| 13/13 [00:07<00:00,  1.67it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m0qGetIhfUE5"
      },
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MX92dRP9q6zj",
        "colab": {}
      },
      "source": [
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "                loss = criterion(logits, y_batch.view(-1))  # my\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward(retain_graph=True)\n",
        "                    optimizer.step()\n",
        "\n",
        "                mask = (y_batch != 0)\n",
        "                \n",
        "                labels = torch.argmax(logits.view(y_batch.size()[0], y_batch.size()[1], -1), dim=2)  # my\n",
        "                cur_correct_count, cur_sum_count = float(torch.sum((labels == y_batch) * mask)), float(mask.sum())  # my\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6vzHxX7qeKq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "1b2fa371-ac97-4418-f760-57136d62e5ef"
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=10,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 10] Train: Loss = 0.73205, Accuracy = 75.95%: 100%|██████████| 572/572 [01:01<00:00,  9.24it/s]\n",
            "[1 / 10]   Val: Loss = 0.38562, Accuracy = 87.04%: 100%|██████████| 13/13 [00:08<00:00,  1.66it/s]\n",
            "[2 / 10] Train: Loss = 0.31187, Accuracy = 89.47%: 100%|██████████| 572/572 [01:02<00:00,  9.18it/s]\n",
            "[2 / 10]   Val: Loss = 0.26684, Accuracy = 90.85%: 100%|██████████| 13/13 [00:07<00:00,  1.75it/s]\n",
            "[3 / 10] Train: Loss = 0.22774, Accuracy = 92.14%: 100%|██████████| 572/572 [00:59<00:00,  9.58it/s]\n",
            "[3 / 10]   Val: Loss = 0.22237, Accuracy = 92.08%: 100%|██████████| 13/13 [00:08<00:00,  1.48it/s]\n",
            "[4 / 10] Train: Loss = 0.18731, Accuracy = 93.31%: 100%|██████████| 572/572 [01:00<00:00,  9.40it/s]\n",
            "[4 / 10]   Val: Loss = 0.20906, Accuracy = 91.78%: 100%|██████████| 13/13 [00:07<00:00,  1.71it/s]\n",
            "[5 / 10] Train: Loss = 0.16233, Accuracy = 94.02%: 100%|██████████| 572/572 [01:00<00:00, 11.66it/s]\n",
            "[5 / 10]   Val: Loss = 0.20793, Accuracy = 92.24%: 100%|██████████| 13/13 [00:07<00:00,  1.95it/s]\n",
            "[6 / 10] Train: Loss = 0.14525, Accuracy = 94.51%: 100%|██████████| 572/572 [01:01<00:00,  9.85it/s]\n",
            "[6 / 10]   Val: Loss = 0.18761, Accuracy = 92.50%: 100%|██████████| 13/13 [00:08<00:00,  1.70it/s]\n",
            "[7 / 10] Train: Loss = 0.13379, Accuracy = 94.82%: 100%|██████████| 572/572 [01:00<00:00,  9.43it/s]\n",
            "[7 / 10]   Val: Loss = 0.19585, Accuracy = 92.62%: 100%|██████████| 13/13 [00:07<00:00,  1.92it/s]\n",
            "[8 / 10] Train: Loss = 0.12540, Accuracy = 95.04%: 100%|██████████| 572/572 [01:00<00:00, 11.47it/s]\n",
            "[8 / 10]   Val: Loss = 0.20128, Accuracy = 92.41%: 100%|██████████| 13/13 [00:08<00:00,  1.66it/s]\n",
            "[9 / 10] Train: Loss = 0.11909, Accuracy = 95.21%: 100%|██████████| 572/572 [01:01<00:00, 11.88it/s]\n",
            "[9 / 10]   Val: Loss = 0.19972, Accuracy = 92.79%: 100%|██████████| 13/13 [00:08<00:00,  1.77it/s]\n",
            "[10 / 10] Train: Loss = 0.11478, Accuracy = 95.32%: 100%|██████████| 572/572 [00:59<00:00,  9.57it/s]\n",
            "[10 / 10]   Val: Loss = 0.19246, Accuracy = 92.58%: 100%|██████████| 13/13 [00:08<00:00,  1.63it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nAfV2dEOfHo5"
      },
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEyZZt4ayYtm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_test_accuracy(data, batch_size=32):\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        correct_count = 0\n",
        "        sum_count = 0\n",
        "\n",
        "        for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "            X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "            model.eval()\n",
        "            logits = model(X_batch)\n",
        "\n",
        "            mask = (y_batch != 0)\n",
        "\n",
        "            labels = torch.argmax(logits.view(y_batch.size()[0], y_batch.size()[1], -1), dim=2)  # my\n",
        "            cur_correct_count, cur_sum_count = float(torch.sum((labels == y_batch) * mask)), float(mask.sum())  # my\n",
        "\n",
        "            correct_count += cur_correct_count\n",
        "            sum_count += cur_sum_count\n",
        "\n",
        "    accuracy = correct_count / sum_count\n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EnKA5YP_x1w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5bbc26b2-09f7-417c-acf5-7b7b552fadc4"
      },
      "source": [
        "test_accuracy = get_test_accuracy(data=(X_test, y_test))\n",
        "print(f\"test_accuracy = {test_accuracy:0.2%}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_accuracy = 92.76%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PXUTSFaEHbDG"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJjILFQSUn0p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b91c9d9a-74a0-47ad-df74-c25803007791"
      },
      "source": [
        "len(word2ind)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45441"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8COtGazgfjp8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "0ae27d5d-e2c5-4147-b962-46caf6205ed3"
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()    \n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        \n",
        "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        \n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, bidirectional=True)\n",
        "        \n",
        "        self.hidden2tag = nn.Linear(lstm_hidden_dim*2, tagset_size)\n",
        "       \n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.word_embeddings(inputs)\n",
        "        lstm_out, _ = self.lstm(\n",
        "            embeds.view(inputs.numel(), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(inputs.numel(), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores\n",
        "\n",
        "\n",
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind),\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=10,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 10] Train: Loss = 0.67030, Accuracy = 77.72%: 100%|██████████| 572/572 [01:52<00:00,  5.06it/s]\n",
            "[1 / 10]   Val: Loss = 0.36424, Accuracy = 87.97%: 100%|██████████| 13/13 [00:16<00:00,  1.08s/it]\n",
            "[2 / 10] Train: Loss = 0.29412, Accuracy = 90.03%: 100%|██████████| 572/572 [01:51<00:00,  5.12it/s]\n",
            "[2 / 10]   Val: Loss = 0.25422, Accuracy = 91.20%: 100%|██████████| 13/13 [00:15<00:00,  1.14s/it]\n",
            "[3 / 10] Train: Loss = 0.21602, Accuracy = 92.43%: 100%|██████████| 572/572 [01:53<00:00,  5.05it/s]\n",
            "[3 / 10]   Val: Loss = 0.21382, Accuracy = 92.58%: 100%|██████████| 13/13 [00:16<00:00,  1.04s/it]\n",
            "[4 / 10] Train: Loss = 0.17799, Accuracy = 93.55%: 100%|██████████| 572/572 [01:50<00:00,  5.17it/s]\n",
            "[4 / 10]   Val: Loss = 0.19139, Accuracy = 93.19%: 100%|██████████| 13/13 [00:15<00:00,  1.21s/it]\n",
            "[5 / 10] Train: Loss = 0.15501, Accuracy = 94.22%: 100%|██████████| 572/572 [01:52<00:00,  5.11it/s]\n",
            "[5 / 10]   Val: Loss = 0.18209, Accuracy = 93.13%: 100%|██████████| 13/13 [00:14<00:00,  1.21s/it]\n",
            "[6 / 10] Train: Loss = 0.13998, Accuracy = 94.63%: 100%|██████████| 572/572 [01:51<00:00,  5.12it/s]\n",
            "[6 / 10]   Val: Loss = 0.18118, Accuracy = 92.56%: 100%|██████████| 13/13 [00:15<00:00,  1.03s/it]\n",
            "[7 / 10] Train: Loss = 0.12944, Accuracy = 94.89%: 100%|██████████| 572/572 [01:52<00:00,  5.08it/s]\n",
            "[7 / 10]   Val: Loss = 0.17134, Accuracy = 93.36%: 100%|██████████| 13/13 [00:14<00:00,  1.17s/it]\n",
            "[8 / 10] Train: Loss = 0.12169, Accuracy = 95.11%: 100%|██████████| 572/572 [01:51<00:00,  5.14it/s]\n",
            "[8 / 10]   Val: Loss = 0.16884, Accuracy = 92.82%: 100%|██████████| 13/13 [00:16<00:00,  1.04s/it]\n",
            "[9 / 10] Train: Loss = 0.11634, Accuracy = 95.25%: 100%|██████████| 572/572 [01:50<00:00,  5.20it/s]\n",
            "[9 / 10]   Val: Loss = 0.17240, Accuracy = 92.85%: 100%|██████████| 13/13 [00:14<00:00,  1.07s/it]\n",
            "[10 / 10] Train: Loss = 0.11236, Accuracy = 95.38%: 100%|██████████| 572/572 [01:51<00:00,  5.12it/s]\n",
            "[10 / 10]   Val: Loss = 0.16435, Accuracy = 93.74%: 100%|██████████| 13/13 [00:15<00:00,  1.01it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Td2SYbMMLc1J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1bf861e9-0bc5-4296-b85e-dd2ea6020518"
      },
      "source": [
        "test_accuracy = get_test_accuracy(data=(X_test, y_test))\n",
        "print(f\"test_accuracy = {test_accuracy:0.2%}\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_accuracy = 93.76%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iWPOdrVOB_c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e91b60b-27ec-4437-d0ef-399634522553"
      },
      "source": [
        "# one more variant for the test accuracy\n",
        "\n",
        "test_batch_size = 32\n",
        "test_data = (X_test, y_test)\n",
        "\n",
        "test_loss, test_acc = do_epoch(model, criterion, test_data, test_batch_size, None, 'Test:')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test: Loss = 0.16443, Accuracy = 93.71%: 100%|██████████| 448/448 [00:20<00:00, 21.98it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZTXmYGD_ANhm"
      },
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uZpY_Q1xZ18h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b896eda4-ccc8-4858-bd1f-0d7356fe6b1a"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KYogOoKlgtcf"
      },
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VsCstxiO03oT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8cc8216-2b2d-4ff0-a469-fa24bd60a9d6"
      },
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HcG7i-R8hbY3"
      },
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DMOtTrfQ2GP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "599c30a0-3f57-4f0d-bf43-c545477b011b"
      },
      "source": [
        "embeddings.shape"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45441, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdgFmYFeRmWe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "f1e77cdf-ba13-480a-a3a4-34387e8a1555"
      },
      "source": [
        "embeddings"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 1.19920003, -0.074682  , -0.52087998, ..., -1.17509997,\n",
              "         0.31558001,  1.00240004],\n",
              "       [-0.77667999,  0.37514001,  0.38973999, ...,  0.17705999,\n",
              "         0.65305001,  0.47819   ],\n",
              "       ...,\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [-0.0088543 , -0.080168  , -0.46946001, ...,  0.34031001,\n",
              "         0.12179   , -0.14158   ],\n",
              "       [ 0.024539  ,  0.79429001, -0.10781   , ...,  0.36603001,\n",
              "        -0.044822  ,  0.090384  ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LxaRBpQd0pat",
        "colab": {}
      },
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()    \n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        \n",
        "        self.word_embeddings = nn.Embedding.from_pretrained(FloatTensor(embeddings))\n",
        "        \n",
        "        word_emb_dim = embeddings.shape[1]\n",
        "        \n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, bidirectional=True)\n",
        "        \n",
        "        self.hidden2tag = nn.Linear(lstm_hidden_dim*2, tagset_size)\n",
        "       \n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.word_embeddings(inputs)\n",
        "        lstm_out, _ = self.lstm(\n",
        "            embeds.view(inputs.numel(), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(inputs.numel(), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EBtI6BDE-Fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "075cf605-7a9e-4cbf-c416-bab8f808afff"
      },
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=embeddings,\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=10,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 10] Train: Loss = 0.64430, Accuracy = 81.06%: 100%|██████████| 572/572 [01:50<00:00,  5.17it/s]\n",
            "[1 / 10]   Val: Loss = 0.34937, Accuracy = 89.38%: 100%|██████████| 13/13 [00:16<00:00,  1.16s/it]\n",
            "[2 / 10] Train: Loss = 0.27557, Accuracy = 91.40%: 100%|██████████| 572/572 [01:48<00:00,  5.27it/s]\n",
            "[2 / 10]   Val: Loss = 0.26527, Accuracy = 91.66%: 100%|██████████| 13/13 [00:16<00:00,  1.15s/it]\n",
            "[3 / 10] Train: Loss = 0.22375, Accuracy = 92.68%: 100%|██████████| 572/572 [01:50<00:00,  5.19it/s]\n",
            "[3 / 10]   Val: Loss = 0.23352, Accuracy = 92.43%: 100%|██████████| 13/13 [00:15<00:00,  1.04s/it]\n",
            "[4 / 10] Train: Loss = 0.19869, Accuracy = 93.30%: 100%|██████████| 572/572 [01:50<00:00,  5.19it/s]\n",
            "[4 / 10]   Val: Loss = 0.21541, Accuracy = 92.63%: 100%|██████████| 13/13 [00:15<00:00,  1.15s/it]\n",
            "[5 / 10] Train: Loss = 0.18404, Accuracy = 93.62%: 100%|██████████| 572/572 [01:50<00:00,  5.17it/s]\n",
            "[5 / 10]   Val: Loss = 0.20443, Accuracy = 93.08%: 100%|██████████| 13/13 [00:15<00:00,  1.01s/it]\n",
            "[6 / 10] Train: Loss = 0.17483, Accuracy = 93.81%: 100%|██████████| 572/572 [01:51<00:00,  5.15it/s]\n",
            "[6 / 10]   Val: Loss = 0.19708, Accuracy = 93.24%: 100%|██████████| 13/13 [00:16<00:00,  1.12s/it]\n",
            "[7 / 10] Train: Loss = 0.16868, Accuracy = 93.96%: 100%|██████████| 572/572 [01:49<00:00,  5.24it/s]\n",
            "[7 / 10]   Val: Loss = 0.19506, Accuracy = 92.98%: 100%|██████████| 13/13 [00:15<00:00,  1.17s/it]\n",
            "[8 / 10] Train: Loss = 0.16399, Accuracy = 94.06%: 100%|██████████| 572/572 [01:51<00:00,  5.14it/s]\n",
            "[8 / 10]   Val: Loss = 0.19066, Accuracy = 93.23%: 100%|██████████| 13/13 [00:16<00:00,  1.13s/it]\n",
            "[9 / 10] Train: Loss = 0.16076, Accuracy = 94.11%: 100%|██████████| 572/572 [01:49<00:00,  5.24it/s]\n",
            "[9 / 10]   Val: Loss = 0.18746, Accuracy = 93.28%: 100%|██████████| 13/13 [00:16<00:00,  1.30s/it]\n",
            "[10 / 10] Train: Loss = 0.15734, Accuracy = 94.17%: 100%|██████████| 572/572 [01:50<00:00,  5.16it/s]\n",
            "[10 / 10]   Val: Loss = 0.18501, Accuracy = 93.36%: 100%|██████████| 13/13 [00:14<00:00,  1.04s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Ne_8f24h8kg"
      },
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HPUuAPGhEGVR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e930b37-93ad-48ff-bac4-d7453b92b081"
      },
      "source": [
        "test_batch_size = 32\n",
        "test_data = (X_test, y_test)\n",
        "\n",
        "test_loss, test_acc = do_epoch(model, criterion, test_data, test_batch_size, None, 'Test:')"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test: Loss = 0.18466, Accuracy = 93.42%: 100%|██████████| 448/448 [00:20<00:00, 21.35it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}